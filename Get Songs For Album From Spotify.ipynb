{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f047ef-d67f-404c-960f-75381f3d9992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Volumes/workspace/music/albums-supplemental/SupplementalAlbums1.csv\", header=True, inferSchema=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99f2bf4-ee27-49de-9ad5-f144578f8b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CLIENT_ID = dbutils.secrets.get(scope = \"Spotify\", key = \"client-id\")\n",
    "CLIENT_SECRET = dbutils.secrets.get(scope = \"Spotify\", key = \"client-secret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79380094-b1bd-4f58-8158-5d348bd247a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "def get_token():\n",
    "    url = \"https://accounts.spotify.com/api/token\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data={\"grant_type\": \"client_credentials\"},\n",
    "        auth=(CLIENT_ID, CLIENT_SECRET),\n",
    "    )\n",
    "    return response.json()[\"access_token\"]\n",
    "\n",
    "def get_album_tracks(album_num, artist_name, album_name, year, token=None):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query = f\"artist:{artist_name},album:{album_name}\"\n",
    "    encoded_query = urllib.parse.quote_plus(query)\n",
    "\n",
    "    # Search album\n",
    "    search_url = f\"https://api.spotify.com/v1/search?q={encoded_query}&type=album&limit=7\"\n",
    "    print(search_url)\n",
    "    search_res = requests.get(search_url, headers=headers).json()\n",
    "    \n",
    "    if not search_res[\"albums\"][\"items\"]:\n",
    "        print (f\"No albums found for {album_name} by {artist_name}.\")\n",
    "        return None\n",
    "    \n",
    "    if len(search_res[\"albums\"][\"items\"]) > 1:\n",
    "        print(f\"Multiple albums found for {album_name} by {artist_name}.\")\n",
    "        # Print each possibility and ask for user input\n",
    "        print(f\"0. None of the below\")\n",
    "        for i, item in enumerate(search_res[\"albums\"][\"items\"], 1):\n",
    "            print(f\"{i}. {item['name']} ({item['release_date']})\")\n",
    "        choice = int(input(\"Enter the number of the correct album: \"))\n",
    "        if choice == 0:\n",
    "            return None\n",
    "        album = search_res[\"albums\"][\"items\"][choice-1]\n",
    "    else:\n",
    "        album = search_res[\"albums\"][\"items\"][0]\n",
    "\n",
    "    album_id = album[\"id\"]\n",
    "    release_date = album[\"release_date\"]\n",
    "    release_date_precision = album[\"release_date_precision\"]\n",
    "    print(f\"{release_date} {release_date_precision}\")\n",
    "\n",
    "    # Get tracks\n",
    "    tracks_url = f\"https://api.spotify.com/v1/albums/{album_id}/tracks\"\n",
    "    tracks_res = requests.get(tracks_url, headers=headers).json()\n",
    "    \n",
    "    tracks_info = [(album_num, track[\"track_number\"], track[\"name\"], track[\"duration_ms\"]) for track in tracks_res[\"items\"]]\n",
    "    tracks_df = spark.createDataFrame(tracks_info, [\"album_num\", \"track_number\", \"name\", \"duration_ms\"])\n",
    "    return tracks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41cd500e-c331-47f3-8820-f714093c8658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "token = get_token()\n",
    "volume_path = \"/Volumes/workspace/music/albums-supplemental\"\n",
    "\n",
    "index = 0\n",
    "for artist, album, year in df.take(100):\n",
    "    index += 1\n",
    "    file_name = f\"{(3000+index):04d}-{artist}-{album}.csv\"\n",
    "    file_path = f\"{volume_path}/{file_name}\"\n",
    "\n",
    "    # Check to see if the file exists in the tracks volume already\n",
    "    try:\n",
    "        files = dbutils.fs.ls(file_path)\n",
    "        print(f\"Skipping {file_name}...\")\n",
    "        continue\n",
    "    except Exception:\n",
    "        pass  # File does not exist, proceed\n",
    "\n",
    "    tracks_df = get_album_tracks((3000+index), artist, album, year, token)\n",
    "    if not tracks_df:\n",
    "        continue\n",
    "\n",
    "    print(tracks_df.head(3))\n",
    "    print()\n",
    "\n",
    "    # Write the tracks to the tracks volume\n",
    "    tracks_df.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(f\"{volume_path}/{file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a686de-0c27-49a0-a79e-7e5c88c38355",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "volume_path = \"/Volumes/workspace/music/albums-1001-tracks\"\n",
    "\n",
    "# List all files in the volume path\n",
    "files = dbutils.fs.ls(volume_path)\n",
    "\n",
    "for file_info in files:\n",
    "    if not file_info.isDir():\n",
    "        file_path = file_info.path\n",
    "        \n",
    "        # Check if the folder name ends with .csv.csv.csv\n",
    "        if file_path.endswith(\".temp.csv\"):\n",
    "            dbutils.fs.mv(file_path, file_path.replace(\".temp.csv\", \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7b0be0-3507-4ad9-9edb-a919c290a0c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "volume_path = \"/Volumes/workspace/music/albums-1001-tracks\"\n",
    "\n",
    "# List all files in the volume path\n",
    "files = dbutils.fs.ls(volume_path)\n",
    "\n",
    "for file_info in files:\n",
    "    if file_info.isDir():\n",
    "        folder_path = file_info.path\n",
    "        folder_name = os.path.basename(folder_path.rstrip('/'))\n",
    "        \n",
    "        # Check if the folder name ends with .csv.csv.csv\n",
    "        if folder_name.startswith(\"002\"):\n",
    "            print(f\"Deleting directory: {folder_path}\")\n",
    "            dbutils.fs.rm(folder_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0bafb4a-bd45-45bd-89cc-14d13b4a9aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "volume_path = \"/Volumes/workspace/music/albums-1001-tracks\"\n",
    "\n",
    "# List all files in the volume path\n",
    "files = dbutils.fs.ls(volume_path)\n",
    "\n",
    "for file_info in files:\n",
    "    if file_info.isDir():\n",
    "        folder_path = file_info.path\n",
    "        folder_name = os.path.basename(folder_path.rstrip('/'))\n",
    "        \n",
    "        # Check if the folder name ends with .csv\n",
    "        if folder_name.endswith(\".csv\"):\n",
    "            new_folder_name = folder_name[:-4]\n",
    "            new_folder_path = os.path.join(os.path.dirname(folder_path.rstrip('/')), new_folder_name)\n",
    "            print(f\"Renaming directory: {folder_path} to {new_folder_path}\")\n",
    "            dbutils.fs.mv(folder_path, new_folder_path, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19d08637-692c-44f2-b5c1-1d3c4d303883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "volume_path = \"/Volumes/workspace/music/albums-supplemental\"\n",
    "\n",
    "files = dbutils.fs.ls(volume_path)\n",
    "\n",
    "for file_info in files:\n",
    "    if file_info.isDir():\n",
    "        folder_path = file_info.path\n",
    "        folder_name = folder_path.rstrip('/').split('/')[-1]\n",
    "        \n",
    "        print(f\"Processing directory: {folder_path}\")\n",
    "        df = spark.read.csv(folder_path, header=True, inferSchema=True)\n",
    "        temp_csv_path = f\"{os.path.dirname(folder_path.rstrip('/'))}/{folder_name}.temp\"\n",
    "        single_csv_path = temp_csv_path[:-5] + \".csv\"\n",
    "        \n",
    "        # Write Spark DataFrame directly to DBFS as a single CSV file\n",
    "        df.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(temp_csv_path)\n",
    "\n",
    "        temporary_csv = os.path.join(temp_csv_path, dbutils.fs.ls(temp_csv_path)[3][1])\n",
    "\n",
    "        dbutils.fs.cp(temporary_csv, single_csv_path)\n",
    "        print(f\"Written single CSV file: {single_csv_path}\")\n",
    "        \n",
    "        print(f\"Deleting directory: {temp_csv_path}\")\n",
    "        dbutils.fs.rm(temp_csv_path, True)\n",
    "        print(f\"Deleting directory: {folder_path}\")\n",
    "        dbutils.fs.rm(folder_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b773a63-ecc5-4290-90a8-6e767a27e2aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    # Example for moving multiple files using a loop\n",
    "    source_base_path = \"/Volumes/workspace/music/albums-supplemental/\"\n",
    "    dest_base_path = \"/Volumes/workspace/music/albums-supplemental-tracks/\"\n",
    "\n",
    "    for file_name in [file.name for file in dbutils.fs.ls(volume_path) if file.name.endswith('.csv')]:\n",
    "        source_path = source_base_path + file_name\n",
    "        dest_path = dest_base_path + file_name\n",
    "        dbutils.fs.mv(source_path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b9dc9f-949f-476b-99ae-846184620db7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "volume_path = \"/Volumes/workspace/music/albums-supplemental-tracks\"\n",
    "\n",
    "# List all files in the volume path\n",
    "files = dbutils.fs.ls(volume_path)\n",
    "\n",
    "for file_info in files:\n",
    "    if not file_info.isDir():\n",
    "        file_path = file_info.path\n",
    "        \n",
    "        # Check if the file name ends with .csv.csv\n",
    "        if file_path.endswith(\".csv.csv\"):\n",
    "            new_file_path = file_path[:-4]\n",
    "            print(f\"Renaming file: {file_path} to {new_file_path}\")\n",
    "            dbutils.fs.mv(file_path, new_file_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8614230561804224,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Get Songs For Album From Spotify",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
